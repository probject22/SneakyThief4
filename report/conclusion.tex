Starting from creating a 2-D world with agents randomly moving around, the simulator is developed to a level where the intruder finds it's way to the target smartly by avoiding the guards and agents would work together to learn the map, to patrol and catch the thief using different methods and algorithms. Several algorithms were implemented and tested for each functionality of the simulator such as path-finding, exploration, patrolling, pursuit and avoidance. One main success of this development is the two innovative algorithms, a path-finding and an avoidance algorithm namely; ARob and SneakyEscape. The results achieved from each experiment exposed various behaviours of the algorithms when applied to the simulator. The path finding experiment proved the precision of A-star to find the best possible root. However, A-star showed a poor behaviour in running time when compared to the ARob algorithm. Patrolling experiments revealed that the exploration algorithm which do not move to a random location only perform significantly better at long term simulation run. Due to time limitations, the pursuit and the avoidance experiments were performed for 100 maps, which is not sufficient to achieve a significant comparison between different algorithms. Nevertheless, the pursuit experiment showed a 47\% increase in guards success rate after using the BES algorithm. 
    Overall, the simulator is still at the a developing stage and plenty of features and potentials of it need to be explored. For instance, the simulator does not accommodate for various visibility conditions, such as shadows and shrubbery. This is, however, not a serious limitation to the evaluated algorithms as none of these algorithms have the capabilities to perform intelligent behaviour based on such world conditions. This is a topic that future research could attend to. Moreover, although sentry towers did exist in our simulations, any use of them by any of the algorithms employed currently is purely incidental.
    Further limitations of the present study are mostly due to implementation deficiencies. The RTTEh, and subsequently MTES, algorithms we elaborately discussed, but only partially implemented, so that no experiments could be performed.
    Future research can, as already mentioned, investigate algorithms that are capable of anticipating more complex features of the surroundings, such as the possibility to hide, enter sentry towers, look through windows, predict moving directions of the other agents, and employ more advanced communication among agents. Also additional sensory elements could be utilised, such as a more elaborate interpretation of sounds, and possibly other tracks left behind by agents. Techniques to handle such complexity shall probably include those prescribed by machine learning, such as Q-learning, or Neural Networks.
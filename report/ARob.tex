This algorithm is a modified version of A* algorithm. The main difference is that it gets a penalty by revisiting a cell.  his can be seen in eq.~\ref{eq:ARob}. This evaluation function is bassed on A*'s evaluation function (eq.~\ref{eq:astarevaluation}). The $p(x)$ is the penalty for revisiting a cell. The idea behind this is that it still can go back to a spot where it was before, but it avoids an agents oscillating between two spots. Alg.~2 shows the pseudocode of this algorithm.

\begin{equation}
	\label{eq:ARob}
 	f(x) = g(x) + h(x) + p(x)
\end{equation}


\begin{algorithm}[h!]
    \label{al:ARob}
    \caption{Single iteration of ARob}
    \KwData{Current location $c$; Set of possible goal coordinate $G$; List of penalties $p(x)$ at coordinate $x$; List of expected costs $f(x)$ at coordinate }
    \If{ $c \in G$}{
        \Return found path\;
    }
    \If{$G$ has changed}{
        reset $p$\;
    }
    Increment $p(c)$\;
    \For{ $l \in \mathrm{neighbours}(c)$ }{
        $f(l) \leftarrow \mathrm{heuristic}(G) + \mathrm{movecost}(l) + p(l)$
    }
    
    move to $\min_f \mathrm{neighbours}(c)$
\end{algorithm}

% \begin{lstlisting}
%     while(true){
%         if(currentspot == goal){
%             Stop;
%         }
        
%         if (goal has changed){
%             reset penalty-list;
%         }
        
%         penalty-list.currentSpot += penaly;
        
%         Neighbours == getNeighbors(currentSpot)
        
%         for(neighbor : neighbours){
%             neighbor.f = expectedDistence + moveCost + penalty-list.neighbor
%         }
        
%         moveTo = neighbours.lowest.f;
%     }
    
% \end{lstlisting}